{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we compare the GPyTorch fast posteriors with the decoupled samplers\n",
    "(from https://arxiv.org/abs/2002.09309), using the BoTorch based implementation\n",
    "available in this repo.\n",
    "\n",
    "This is partly based on the GPyTorch notebook \"GP Regression with LOVE for Fast\n",
    "Predictive Variances and Sampling\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from typing import Any, Union, Optional, List\n",
    "import torch\n",
    "import gpytorch\n",
    "from botorch.posteriors import GPyTorchPosterior\n",
    "from gpytorch.likelihoods import FixedNoiseGaussianLikelihood\n",
    "from torch import Tensor\n",
    "from gp_sampling import decoupled_sampler\n",
    "from botorch.test_functions import Hartmann, Ackley\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch import fit_gpytorch_model\n",
    "from gpytorch.settings import fast_pred_var\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Uses `num_train` points from 6D Hartmann function, randomly drawn from [0, 1]^6\n",
    "\n",
    "Play around with `num_train` and `num_test` to see how the runtime changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 100\n",
    "num_test = 5000\n",
    "dim=6\n",
    "train_x = torch.rand(num_train, dim).contiguous()\n",
    "function = Hartmann(dim=dim, noise_std=0.1)\n",
    "# Ackley was used for debugging.\n",
    "# function = Ackley(dim=dim, noise_std=1)\n",
    "train_y = function(train_x).unsqueeze(-1).contiguous()\n",
    "# It appears that the implementation is not fully compatible with the outcome\n",
    "# transforms. Thus, we will manually standardize train_y.\n",
    "standardize_mean = train_y.mean()\n",
    "standardize_std = train_y.std()\n",
    "train_y = (train_y - standardize_mean) / standardize_std\n",
    "\n",
    "test_x = torch.rand(num_test, dim).contiguous()\n",
    "\n",
    "if torch.cuda.is_available() and False:  # disable cuda for now\n",
    "    train_x, train_y, test_x = train_x.cuda(), train_y.cuda(), test_x.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GP Model\n",
    "\n",
    "We now define the GP model. Here, we use a stripped version of the  `SingleTaskGP` model\n",
    "from BoTorch. The `StrippedGP` model removes all the built in GPyTorch context managers\n",
    " from the `SingleTaskGP` relating to posterior computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class StrippedPosterior(GPyTorchPosterior):\n",
    "    def rsample(\n",
    "        self,\n",
    "        sample_shape: Optional[torch.Size] = None,\n",
    "        base_samples: Optional[Tensor] = None,\n",
    "    ) -> Tensor:\n",
    "        if sample_shape is None:\n",
    "            sample_shape = torch.Size([1])\n",
    "        if base_samples is not None:\n",
    "            if base_samples.shape[: len(sample_shape)] != sample_shape:\n",
    "                raise RuntimeError(\"sample_shape disagrees with shape of base_samples.\")\n",
    "            # get base_samples to the correct shape\n",
    "            base_samples = base_samples.expand(sample_shape + self.event_shape)\n",
    "            # remove output dimension in single output case\n",
    "            if not self._is_mt:\n",
    "                base_samples = base_samples.squeeze(-1)\n",
    "        samples = self.mvn.rsample(\n",
    "            sample_shape=sample_shape, base_samples=base_samples\n",
    "        )\n",
    "        # make sure there always is an output dimension\n",
    "        if not self._is_mt:\n",
    "            samples = samples.unsqueeze(-1)\n",
    "        return samples\n",
    "\n",
    "class StrippedGP(SingleTaskGP):\n",
    "    def posterior(\n",
    "        self,\n",
    "        X: Tensor,\n",
    "        output_indices: Optional[List[int]] = None,\n",
    "        observation_noise: Union[bool, Tensor] = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> StrippedPosterior:\n",
    "        self.eval()  # make sure model is in eval mode\n",
    "        # insert a dimension for the output dimension\n",
    "        if self._num_outputs > 1:\n",
    "            raise NotImplementedError\n",
    "        mvn = self(X)\n",
    "        if observation_noise is not False:\n",
    "            if torch.is_tensor(observation_noise):\n",
    "                # TODO: Validate noise shape\n",
    "                # make observation_noise `batch_shape x q x n`\n",
    "                obs_noise = observation_noise.transpose(-1, -2)\n",
    "                mvn = self.likelihood(mvn, X, noise=obs_noise)\n",
    "            elif isinstance(self.likelihood, FixedNoiseGaussianLikelihood):\n",
    "                # Use the mean of the previous noise values (TODO: be smarter here).\n",
    "                noise = self.likelihood.noise.mean().expand(X.shape[:-1])\n",
    "                mvn = self.likelihood(mvn, X, noise=noise)\n",
    "            else:\n",
    "                mvn = self.likelihood(mvn, X)\n",
    "        posterior = StrippedPosterior(mvn=mvn)\n",
    "        if hasattr(self, \"outcome_transform\"):\n",
    "            posterior = self.outcome_transform.untransform_posterior(posterior)\n",
    "        return posterior\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StrippedGP(train_x, train_y)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "_ = fit_gpytorch_model(mll)  # ignore the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing predictive variances\n",
    "\n",
    "### Using standard computations\n",
    "\n",
    "The next cell gets the predictive mean and covariance for the test set\n",
    "with no acceleration or precomputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute exact mean + covariances: 0.34s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Clear the cache from the previous computations\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "# Set into eval mode\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    posterior = model.posterior(test_x)\n",
    "    exact_mean = posterior.mean\n",
    "    exact_covar = posterior.mvn.covariance_matrix\n",
    "    exact_covar_time = time.time() - start_time\n",
    "    \n",
    "print(f\"Time to compute exact mean + covariances: {exact_covar_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using fast predictive variances\n",
    "\n",
    "Next we compute predictive mean and covariances using `fast_predictive_var` context\n",
    "manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the cache from the previous computations\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "# Set into eval mode\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), fast_pred_var():\n",
    "    start_time = time.time()\n",
    "    posterior = model.posterior(test_x)\n",
    "    _ = posterior.mean\n",
    "    _ = posterior.mvn.covariance_matrix\n",
    "    fast_time_no_cache = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell additionally computed the caches required to get fast predictions. From this point onwards, unless we put the model back in training mode, predictions should be extremely fast. The cell below re-runs the above code, but takes full advantage of both the mean cache and the LOVE cache for variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute mean + covariances (no cache) 0.27s\n",
      "Time to compute mean + variances (cache): 0.29s\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(), fast_pred_var():\n",
    "    start_time = time.time()\n",
    "    posterior = model.posterior(test_x)\n",
    "    fast_mean = posterior.mean\n",
    "    fast_covar = posterior.mvn.covariance_matrix\n",
    "    fast_time_with_cache = time.time() - start_time\n",
    "\n",
    "print('Time to compute mean + covariances (no cache) {:.2f}s'.format(fast_time_no_cache))\n",
    "print('Time to compute mean + variances (cache): {:.2f}s'.format(fast_time_with_cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Error between Exact and Fast Variances\n",
    "\n",
    "Finally, we compute the mean absolute error between the fast variances computed by LOVE (stored in fast_covar), and the exact variances computed previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAE between exact covar matrix and fast covar matrix: nan\n"
     ]
    }
   ],
   "source": [
    "smae = ((exact_covar - fast_covar).abs() / exact_covar.abs()).mean()\n",
    "print(f\"SMAE between exact covar matrix and fast covar matrix: {smae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing posterior samples (KISS-GP only)\n",
    "\n",
    "With KISS-GP models, LOVE can also be used to draw fast posterior samples. (The same does not apply to exact GP models.)\n",
    "\n",
    "Since this part here does not use KISS-GP, we cannot utilize `gpytorch.settings\n",
    ".fast_pred_samples()` context manager. We will get back to KISS-GP later.\n",
    "\n",
    "### Drawing samples the standard way (without LOVE)\n",
    "\n",
    "We now draw samples from the posterior distribution. Without LOVE, we accomlish this by performing Cholesky on the posterior covariance matrix. This can be slow for large covariance matrices."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute exact samples: 1.07s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_samples = 2000\n",
    "\n",
    "# Clear the cache from the previous computations\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "# Set into eval mode\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    exact_samples = model.posterior(test_x).rsample(torch.Size([num_samples]))\n",
    "    exact_sample_time = time.time() - start_time\n",
    "    \n",
    "print(f\"Time to compute exact samples: {exact_sample_time:.2f}s\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `fast_pred_var()`\n",
    "\n",
    "Since we do not use KISS-GP here, we cannot use `with gpytorch.settings\n",
    ".fast_pred_samples():` to speed up posterior sampling.\n",
    "\n",
    "We will still use `with gpytorch.settings.fast_pred_var():` flag to speed up the\n",
    "computation of the covariance matrix.\n",
    "\n",
    "We can also use the `gpytorch.settings.max_root_decomposition_size(10)` setting, which\n",
    " speeds up the computations for large matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute fast samples (no cache) 0.88s\n",
      "Time to compute fast samples (cache) 0.85s\n",
      "Time to compute fast samples (small root, no cache) 0.29s\n",
      "Time to compute fast samples (small root, cache) 0.28s\n"
     ]
    }
   ],
   "source": [
    "# Clear the cache from the previous computations\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "# Set into eval mode\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "# Repeat the timing now that the cache is computed\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    start_time = time.time()\n",
    "    _ = model.posterior(test_x).rsample(torch.Size([num_samples]))\n",
    "    fast_sample_time_no_cache = time.time() - start_time\n",
    "\n",
    "# Repeat the timing now that the cache is computed\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    start_time = time.time()\n",
    "    _ = model.posterior(test_x).rsample(torch.Size([num_samples]))\n",
    "    fast_sample_time_cache = time.time() - start_time\n",
    "\n",
    "# Clear the cache from the previous computations\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "# Set into eval mode\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(),\\\n",
    "     gpytorch.settings.max_root_decomposition_size(10):\n",
    "    start_time = time.time()\n",
    "    _ = model.posterior(test_x).rsample(torch.Size([num_samples]))\n",
    "    small_root_time_no_cache = time.time() - start_time\n",
    "    \n",
    "# Repeat the timing now that the cache is computed\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(),\\\n",
    "     gpytorch.settings.max_root_decomposition_size(10):\n",
    "    start_time = time.time()\n",
    "    love_samples = model.posterior(test_x).rsample(torch.Size([num_samples]))\n",
    "    small_root_time_cache = time.time() - start_time\n",
    "    \n",
    "print('Time to compute fast samples (no cache) {:.2f}s'.format(fast_sample_time_no_cache))\n",
    "print('Time to compute fast samples (cache) {:.2f}s'.format(fast_sample_time_cache))\n",
    "print('Time to compute fast samples (small root, no cache) {:.2f}s'.format\n",
    "      (small_root_time_no_cache))\n",
    "print('Time to compute fast samples (small root, cache) {:.2f}s'.format\n",
    "      (small_root_time_cache))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Draw samples using the decoupled samplers\n",
    "\n",
    "Here, we use the decoupled samplers to sample from the posterior.\n",
    "\n",
    "The loop over `num_basis` is to see how increasing the number of basis functions affects\n",
    " the sampler construction and sampling times.\n",
    "\n",
    "Note that the time to construct the sampler does not depend on `num_samples`. So, we\n",
    "could draw additional samples along the same \"sample path\" without having to\n",
    "reconstruct the sampler. Drawing samples from the sampler does not require much more\n",
    "than some `matmul` operations.\n",
    "\n",
    "Constructing the sampler requires the Cholesky decomposition of the `num_train x\n",
    "num_train` kernel matrix, which explains increase in initialization time as `num_train`\n",
    "increases. Wilson also introduces a sparse GP implementation of decoupled samplers,\n",
    "which would help with this issue. The sparse GP version is not implemented here (yet)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing decoupled sampler with 64 basis functions took 0.04 s\n",
      "Drawing decoupled samples with 64 basis functions took 0.06 s\n",
      "Initializing decoupled sampler with 256 basis functions took 0.04 s\n",
      "Drawing decoupled samples with 256 basis functions took 0.07 s\n",
      "Initializing decoupled sampler with 1024 basis functions took 0.07 s\n",
      "Drawing decoupled samples with 1024 basis functions took 0.17 s\n",
      "Initializing decoupled sampler with 4096 basis functions took 0.14 s\n",
      "Drawing decoupled samples with 4096 basis functions took 0.71 s\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 2000\n",
    "# sampling from too many points at once causes memory issues. We can draw them in mini\n",
    "# batches instead.\n",
    "num_batches = ceil(num_test / float(mini_batch_size))\n",
    "\n",
    "for num_basis in [64, 256, 1024, 4096]:\n",
    "    start_time = time.time()\n",
    "    sampler = decoupled_sampler(model, sample_shape=[num_samples], num_basis=num_basis)\n",
    "    print(\"Initializing decoupled sampler with %d basis functions took %.2f s\" %\n",
    "          (num_basis, time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    decoupled_samples = torch.empty(num_samples, num_test, 1)\n",
    "    for i in range(num_batches):\n",
    "        l_idx = i * mini_batch_size\n",
    "        r_idx = (i+1) * mini_batch_size if i != num_batches - 1 else num_test\n",
    "        decoupled_samples[:, l_idx: r_idx] = sampler(test_x[l_idx: r_idx])\n",
    "    print(\"Drawing decoupled samples with %d basis functions took %.2f s\" %\n",
    "          (num_basis, time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the empirical covariance matrices\n",
    "\n",
    "Let's see how well the samples recover the true mean and covariance\n",
    "matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical covariance MAE (Exact samples): 0.007614322006702423\n",
      "Empirical covariance MAE (LOVE samples): 0.01323997974395752\n",
      "Empirical covariance MAE (Decoupled samples): 0.011483925394713879\n",
      "Empirical mean MAE (Exact samples): 0.009403395466506481\n",
      "Empirical mean MAE (LOVE samples): 0.003239282174035907\n",
      "Empirical mean MAE (Decoupled samples): 0.01099607814103365\n"
     ]
    }
   ],
   "source": [
    "# get rid of the final dimension\n",
    "exact_samples = exact_samples.squeeze(-1)\n",
    "love_samples = love_samples.squeeze(-1)\n",
    "decoupled_samples = decoupled_samples.squeeze(-1)\n",
    "\n",
    "# Compute exact posterior covar\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    posterior = model.posterior(test_x)\n",
    "    # this same as model.posterior() except that it returns the mvn directly\n",
    "    mean, covar = posterior.mean, posterior.mvn.covariance_matrix\n",
    "    variance = posterior.variance\n",
    "    mean, variance = mean.squeeze(-1), variance.squeeze(-1)\n",
    "\n",
    "exact_empirical_covar = ((exact_samples - mean).t() @ (exact_samples - mean)) / num_samples\n",
    "love_empirical_covar = ((love_samples - mean).t() @ (love_samples - mean)) / num_samples\n",
    "decoupled_empirical_covar = ((decoupled_samples - mean).t() @ (decoupled_samples -\n",
    "                                                               mean)) / num_samples\n",
    "\n",
    "exact_empirical_error = ((exact_empirical_covar - covar).abs()).mean()\n",
    "love_empirical_error = ((love_empirical_covar - covar).abs()).mean()\n",
    "decoupled_empirical_error = ((decoupled_empirical_covar - covar).abs()).mean()\n",
    "\n",
    "print(f\"Empirical covariance MAE (Exact samples): {exact_empirical_error}\")\n",
    "print(f\"Empirical covariance MAE (LOVE samples): {love_empirical_error}\")\n",
    "print(f\"Empirical covariance MAE (Decoupled samples): {decoupled_empirical_error}\")\n",
    "\n",
    "# compare the posterior mean as well\n",
    "exact_empirical_mean = exact_samples.mean(dim=0)\n",
    "love_empirical_mean = love_samples.mean(dim=0)\n",
    "decoupled_empirical_mean = decoupled_samples.mean(dim=0)\n",
    "\n",
    "exact_mean_error = (exact_empirical_mean - mean).abs().mean()\n",
    "love_mean_error = (love_empirical_mean - mean).abs().mean()\n",
    "decoupled_mean_error = (decoupled_empirical_mean - mean).abs().mean()\n",
    "\n",
    "print(f\"Empirical mean MAE (Exact samples): {exact_mean_error}\")\n",
    "print(f\"Empirical mean MAE (LOVE samples): {love_mean_error}\")\n",
    "print(f\"Empirical mean MAE (Decoupled samples): {decoupled_mean_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the posterior variance is relatively small, we will also look at the scaled mean\n",
    "absolute error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical covariance SMAE (Exact samples): inf\n",
      "Empirical covariance SMAE (LOVE samples): inf\n",
      "Empirical covariance SMAE (Decoupled samples): inf\n"
     ]
    }
   ],
   "source": [
    "exact_smae = ((exact_empirical_covar - covar).abs() / covar.abs()).mean()\n",
    "love_smae = ((love_empirical_error - covar).abs() / covar.abs()).mean()\n",
    "decoupled_smae = ((decoupled_empirical_error - covar).abs() / covar.abs()).mean()\n",
    "\n",
    "print(f\"Empirical covariance SMAE (Exact samples): {exact_smae}\")\n",
    "print(f\"Empirical covariance SMAE (LOVE samples): {love_smae}\")\n",
    "print(f\"Empirical covariance SMAE (Decoupled samples): {decoupled_smae}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the covariances are super small, this returns either inf or some other large\n",
    "number. To put things into perspective, let's check the average posterior variance at test\n",
    " points.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average posterior variance: 0.46045684814453125\n",
      "Mean absolute deviation (Exact samples): 0.226485013961792\n",
      "Mean absolute deviation (Love samples): 0.4081191420555115\n",
      "Mean absolute deviation (Decoupled samples): 0.014375735074281693\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average posterior variance: {variance.mean()}\")\n",
    "print(\"Mean absolute deviation (Exact samples): \"\n",
    "      f\"{(exact_samples.var(dim=0) - variance).abs().mean()}\")\n",
    "print(\"Mean absolute deviation (Love samples): \"\n",
    "      f\"{(love_samples.var(dim=0) - variance).abs().mean()}\")\n",
    "print(\"Mean absolute deviation (Decoupled samples): \"\n",
    "      f\"{(decoupled_samples.var(dim=0) - variance).abs().mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interestingly, the decoupled samples approximate the posterior variance even better\n",
    "than the exact samples. At the same time, it does a poor job of approximating the\n",
    "posterior mean compared to the alternatives.\n",
    "\n",
    "These values are empirical variance of the samples, computed using the empirical\n",
    "mean of these samples. Let's see how things change if we were to use the\n",
    "empirical variances computed w.r.t. the true posterior mean."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average posterior variance: 0.46045684814453125\n",
      "Mean absolute deviation (Exact samples): 0.22644329071044922\n",
      "Mean absolute deviation (Love samples): 0.40812256932258606\n",
      "Mean absolute deviation (Decoupled samples): 0.014377166517078876\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average posterior variance: {variance.mean()}\")\n",
    "print(\"Mean absolute deviation (Exact samples): \"\n",
    "      f\"{(exact_empirical_covar.diag() - variance).abs().mean()}\")\n",
    "print(\"Mean absolute deviation (Love samples): \"\n",
    "      f\"{(love_empirical_covar.diag() - variance).abs().mean()}\")\n",
    "print(\"Mean absolute deviation (Decoupled samples): \"\n",
    "      f\"{(decoupled_empirical_covar.diag() - variance).abs().mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The KISS-GP model\n",
    "\n",
    "We can use the KISS-GP to achieve significant speed-ups over standard GP models. It is\n",
    " not an exact comparison with the decoupled samplers (since it is a different GP model)\n",
    " , but it is still nice to have it as a benchmark.\n",
    "\n",
    "Since the KISS-GP doesn't work well in larger dimensions, we will instead use the SKIP GP."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "if dim <= 4:\n",
    "    # this is the KISS-GP kernel. Doesn't work with dim > 4\n",
    "    grid_size = gpytorch.utils.grid.choose_grid_size(train_x, 1.0)\n",
    "    covar_module = gpytorch.kernels.ScaleKernel(\n",
    "        gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.RBFKernel(), grid_size=grid_size, num_dims=dim\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    # This is the SKIP GP kernel. It is suitable for large dims.\n",
    "    covar_module = gpytorch.kernels.AdditiveStructureKernel(\n",
    "        gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.RBFKernel(), grid_size=128, num_dims=1\n",
    "            )\n",
    "        ), num_dims=dim\n",
    "    )\n",
    "\n",
    "model = StrippedGP(train_x, train_y, covar_module=covar_module)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "_ = fit_gpytorch_model(mll)  # ignore the output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As before, let's compare the time to compute the predictive mean and covariances."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute exact mean + covariances: 1.71s\n",
      "Time to compute mean + covariances (no cache) 1.64s\n",
      "Time to compute mean + variances (cache): 1.64s\n",
      "Time to compute mean + covariances (small root, no cache) 1.70s\n",
      "Time to compute mean + variances (small root, cache): 1.73s\n",
      "SMAE between exact covar matrix and fast covar matrix: inf\n",
      "SMAE between exact covar matrix and small root, fast covar matrix: inf\n"
     ]
    }
   ],
   "source": [
    "# Clear the cache from the previous computations\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "# Set into eval mode\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    posterior = model.posterior(test_x)\n",
    "    exact_mean = posterior.mean\n",
    "    exact_covar = posterior.mvn.covariance_matrix\n",
    "    exact_covar_time = time.time() - start_time\n",
    "\n",
    "print(f\"Time to compute exact mean + covariances: {exact_covar_time:.2f}s\")\n",
    "\n",
    "# Clear the cache from the previous computations\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "# Set into eval mode\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), fast_pred_var():\n",
    "    start_time = time.time()\n",
    "    posterior = model.posterior(test_x)\n",
    "    _ = posterior.mean\n",
    "    _ = posterior.mvn.covariance_matrix\n",
    "    fast_time_no_cache = time.time() - start_time\n",
    "\n",
    "with torch.no_grad(), fast_pred_var():\n",
    "    start_time = time.time()\n",
    "    posterior = model.posterior(test_x)\n",
    "    fast_mean = posterior.mean\n",
    "    fast_covar = posterior.mvn.covariance_matrix\n",
    "    fast_time_with_cache = time.time() - start_time\n",
    "\n",
    "# Clear the cache from the previous computations\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "\n",
    "# Set into eval mode\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), fast_pred_var(), gpytorch.settings.max_root_decomposition_size(10):\n",
    "    start_time = time.time()\n",
    "    posterior = model.posterior(test_x)\n",
    "    _ = posterior.mean\n",
    "    _ = posterior.mvn.covariance_matrix\n",
    "    small_root_no_cache = time.time() - start_time\n",
    "\n",
    "with torch.no_grad(), fast_pred_var(), gpytorch.settings.max_root_decomposition_size(10):\n",
    "    start_time = time.time()\n",
    "    posterior = model.posterior(test_x)\n",
    "    small_root_mean = posterior.mean\n",
    "    small_root_covar = posterior.mvn.covariance_matrix\n",
    "    small_root_with_cache = time.time() - start_time\n",
    "\n",
    "\n",
    "print('Time to compute mean + covariances (no cache) {:.2f}s'.format(fast_time_no_cache))\n",
    "print('Time to compute mean + variances (cache): {:.2f}s'.format(fast_time_with_cache))\n",
    "print('Time to compute mean + covariances (small root, no cache) '\n",
    "      '{:.2f}s'.format(small_root_no_cache))\n",
    "print('Time to compute mean + variances (small root, cache): '\n",
    "      '{:.2f}s'.format(small_root_with_cache))\n",
    "\n",
    "smae = ((exact_covar - fast_covar).abs() / exact_covar.abs()).mean()\n",
    "print(f\"SMAE between exact covar matrix and fast covar matrix: {smae:.6f}\")\n",
    "\n",
    "smae = ((exact_covar - small_root_covar).abs() / exact_covar.abs()).mean()\n",
    "print(f\"SMAE between exact covar matrix and small root, fast covar matrix: {smae:.6f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}